# Qwen-Image-Edit DMD Training Configuration
# Uses LoRA for memory-efficient training with 8-step denoising
# Compatible with DiffSynth-Studio data format

# Model paths (from DiffSynth-Studio checkpoints)
generator_name: /home/ec2-user/SageMaker/efs/Projects/Qwen-Image-Edit-Acceleration/checkpoints/Qwen-Image-Edit-2509-step4000
real_name: /home/ec2-user/SageMaker/efs/Projects/Qwen-Image-Edit-Acceleration/checkpoints/Qwen-Image-Edit-2509-step4000
fake_name: /home/ec2-user/SageMaker/efs/Projects/Qwen-Image-Edit-Acceleration/checkpoints/Qwen-Image-Edit-2509-step4000

# Alternative: use original model
# generator_name: /home/ec2-user/SageMaker/efs/Models/Qwen-Image-Edit-2509
# real_name: /home/ec2-user/SageMaker/efs/Models/Qwen-Image-Edit-2509
# fake_name: /home/ec2-user/SageMaker/efs/Models/Qwen-Image-Edit-2509

# Model type
model_type: qwen_dmd
generator_type: bidirectional
i2v: false

# LoRA configuration
lora_rank: 32
lora_alpha: 32
# Target modules for LoRA - must be nn.Linear layers, not Sequential containers
# Use ".0" suffix for Linear layers inside Sequential (e.g., to_out.0 not to_out)
lora_target_modules: "to_q,to_k,to_v,add_q_proj,add_k_proj,add_v_proj,to_out.0,to_add_out"

# Scheduler
scheduler_mu: 0.8

# Number of denoising steps (scheduler computes timesteps automatically)
# Reduced from 8 to 4 for better memory efficiency during training
num_inference_steps: 4

# Training hyperparameters
num_train_timestep: 1000
ts_schedule: true
ts_schedule_max: false
timestep_shift: 1.0
min_score_timestep: 0

# CFG
guidance_scale: 4.0
real_guidance_scale: 4.0
fake_guidance_scale: 1.0

# Loss
denoising_loss_type: flow
distribution_loss: qwen_dmd
trainer: score_distillation

# Training settings
mixed_precision: true
gradient_checkpointing: true
seed: 0

# Optimizer
lr: 1.0e-04
lr_critic: 2.0e-05
beta1: 0.9
beta2: 0.999
beta1_critic: 0.9
beta2_critic: 0.999
weight_decay: 0.0

# Data configuration (DiffSynth-Studio compatible)
data_path: /home/ec2-user/SageMaker/efs/Projects/Qwen-Image-Edit-Acceleration/data
metadata_path: /home/ec2-user/SageMaker/efs/Projects/Qwen-Image-Edit-Acceleration/data/train_data.csv
data_file_keys: "label_image,cloth_image,model_image"
edit_image_keys: "cloth_image,model_image"
label_image_key: "label_image"
fixed_prompt: "让图2的模特换上图1的下装"
data_max_count: 10000

batch_size: 1
total_batch_size: 32

# Update ratio: train critic more frequently than generator
dfake_gen_update_ratio: 5

# Image dimensions (reduce for memory - original was 1785x1340)
# Start with 512x512 to verify training works, then increase
height: 512
width: 512

# Logging
log_iters: 100
ema_weight: 0.99
ema_start_step: 200
gc_interval: 50
no_save: false
no_visualize: true

# Negative prompt for CFG
negative_prompt: "blurry, low quality, distorted, ugly, bad anatomy, wrong proportions, watermark, text"

# FSDP settings (for distributed training)
# Available wrap strategies:
#   - "size": Size-based wrapping (default, works for any model)
#   - "qwen": Wraps QwenImageTransformerBlock layers (optimal for Qwen DiT)
#   - "transformer": Custom transformer layer wrapping
generator_fsdp_wrap_strategy: qwen
real_score_fsdp_wrap_strategy: qwen
fake_score_fsdp_wrap_strategy: qwen
text_encoder_fsdp_wrap_strategy: size  # Text encoder uses size-based wrapping
text_encoder_cpu_offload: true
real_score_cpu_offload: true  # Offload frozen teacher model to CPU to save GPU memory
sharding_strategy: full
# Sharding strategies: "full", "hybrid_full", "hybrid_zero2", "no_shard"

# Wandb settings
wandb_host: "https://api.wandb.ai/"
wandb_key: "0d32276b8b4b08bb83ecd160d941dba83b3b4975"
wandb_entity: "genai-view"
wandb_project: "qwen-image-edit-dmd"
wandb_save_dir: "./wandb"
disable_wandb: false  # Set to false and fill in key/entity to enable

# resume_ckpt: logs/qwen_image_dmd/checkpoint_model_000200
save_weights_only: false  # to also save optimizer for next checkpoint
checkpoint_dir: checkpoints/

log_every_n_steps: 10    # print detailed losses every n steps (default: 10)
total_steps: 1000